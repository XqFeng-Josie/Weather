"""
åƒç´ ç©ºé—´U-Neté¢„æµ‹è„šæœ¬

ğŸ“‹ æ–‡ä»¶ä½œç”¨:
    å¯¹è®­ç»ƒå¥½çš„åƒç´ ç©ºé—´U-Netæ¨¡å‹è¿›è¡Œé¢„æµ‹ï¼Œç”Ÿæˆè¯„ä¼°æŒ‡æ ‡å’Œå¯è§†åŒ–å›¾ç‰‡ã€‚
    
ğŸ”„ é¢„æµ‹æµç¨‹:
    1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹å’Œé…ç½®
    2. åŠ è½½å½’ä¸€åŒ–å‚æ•°ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
    3. åŠ è½½æµ‹è¯•æ•°æ®
    4. ç”Ÿæˆé¢„æµ‹
    5. è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå½’ä¸€åŒ–ç©ºé—´ + ç‰©ç†ç©ºé—´ï¼‰
    6. ç”Ÿæˆå¯è§†åŒ–ï¼ˆæ—¶é—´åºåˆ—å›¾ + ä¸–ç•Œåœ°å›¾å¯¹æ¯”ï¼‰
    7. ä¿å­˜æ‰€æœ‰ç»“æœ

ğŸ“Š è¾“å‡ºæ–‡ä»¶:
    - prediction_metrics.json: è¯¦ç»†è¯„ä¼°æŒ‡æ ‡
    - y_pred_*.npy: é¢„æµ‹æ•°æ®ï¼ˆå½’ä¸€åŒ– + ç‰©ç†å•ä½ï¼‰
    - y_true_*.npy: çœŸå€¼æ•°æ®
    - timeseries_*.png: æ—¶é—´åºåˆ—å¯¹æ¯”å›¾
    - spatial_comparison_*.png: ä¸–ç•Œåœ°å›¾å¯¹æ¯”å›¾ â­
    - rmse_vs_leadtime_*.png: RMSEéšé¢„æµ‹æ­¥é•¿å˜åŒ–

ğŸ“– ä½¿ç”¨æ–¹æ³•:
    python predict_pixel_unet.py \\
        --model-dir outputs/pixel_unet \\
        --time-slice 2020-02-01:2020-02-10 \\
        --batch-size 32

ğŸ¯ é¢„æœŸæ•ˆæœ:
    åƒç´ ç©ºé—´U-Net:
        - RMSE: 1-3K
        - ç›¸å…³ç³»æ•°: > 0.995
        - SSIM: > 0.995
"""

import argparse
import torch
import numpy as np
import json
import pickle
from pathlib import Path
from tqdm import tqdm

from weatherdiff.unet import WeatherUNet
from weatherdiff.utils import WeatherDataModule, calculate_metrics, format_metrics
from src.visualization import visualize_predictions_improved


def detect_lon_first(data_array):
    """
    åˆ¤æ–­æ•°æ®çš„ç©ºé—´ç»´åº¦é¡ºåºæ˜¯å¦ä¸º (lon, lat)

    Args:
        data_array: xarray DataArray

    Returns:
        True  -> ç©ºé—´é¡ºåºä¸º (lon, lat)
        False -> ç©ºé—´é¡ºåºä¸º (lat, lon)
        None  -> æ— æ³•åˆ¤æ–­ï¼ˆç¼ºå°‘ç»´åº¦åç§°ï¼‰
    """
    dims = list(getattr(data_array, "dims", []))
    if not dims:
        return None

    lat_dim = next((dim for dim in dims if "lat" in dim.lower()), None)
    lon_dim = next((dim for dim in dims if "lon" in dim.lower()), None)

    if lat_dim is None or lon_dim is None:
        return None

    lat_idx = dims.index(lat_dim)
    lon_idx = dims.index(lon_dim)

    return lon_idx < lat_idx


def predict_pixel_unet(args):
    """åƒç´ ç©ºé—´U-Neté¢„æµ‹"""

    model_dir = Path(args.model_dir)
    output_dir = Path(args.output_dir) if args.output_dir else model_dir
    output_dir.mkdir(parents=True, exist_ok=True)

    print("\n" + "=" * 80)
    print("åƒç´ ç©ºé—´U-Neté¢„æµ‹")
    print("=" * 80)
    print(f"æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"é¢„æµ‹æ—¶é—´: {args.time_slice}")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")

    # ========================================================================
    # Step 1: åŠ è½½é…ç½®
    # ========================================================================
    print("\n" + "-" * 80)
    print("Step 1: åŠ è½½é…ç½®")
    print("-" * 80)

    config_path = model_dir / "config.json"
    with open(config_path, "r") as f:
        config = json.load(f)

    print(f"âœ“ åŠ è½½é…ç½®: {config_path}")
    print(f"  è¾“å…¥åºåˆ—é•¿åº¦: {config['input_length']}")
    print(f"  è¾“å‡ºåºåˆ—é•¿åº¦: {config['output_length']}")
    print(f"  å½’ä¸€åŒ–æ–¹æ³•: {config['normalization']}")

    # ä»configè¯»å–è®­ç»ƒæ—¶ä½¿ç”¨çš„ levelsï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    available_levels = config.get("levels", None)
    if available_levels is not None:
        if len(available_levels) == 1:
            print(f"  è®­ç»ƒæ—¶ä½¿ç”¨å•levelæ¨¡å¼: level {available_levels[0]}")
        else:
            print(f"  è®­ç»ƒæ—¶ä½¿ç”¨å¤šlevelæ¨¡å¼: levels {available_levels}")
    else:
        print(f"  è®­ç»ƒæ—¶æœªæŒ‡å®šlevelsï¼ˆä½¿ç”¨æ‰€æœ‰å¯ç”¨levelsæˆ–æ— levelå˜é‡ï¼‰")

    # å¤„ç†å‘½ä»¤è¡ŒæŒ‡å®šçš„ --levels å‚æ•°
    # æ³¨æ„ï¼šå¯¹äºå•levelè®­ç»ƒçš„æ¨¡å‹ï¼Œæ‰€æœ‰é€šé“éƒ½æ¥è‡ªåŒä¸€ä¸ªlevel
    # æ‰€ä»¥ä¸éœ€è¦channel_indicesæ¥é€‰æ‹©ç‰¹å®šé€šé“
    selected_levels = args.levels
    if selected_levels is not None:
        if available_levels is None:
            raise ValueError(
                "No 'levels' found in config. Cannot select specific levels. "
                "Please use all levels (omit --levels argument)."
            )

        # ç¡®ä¿ available_levels æ˜¯åˆ—è¡¨
        if not isinstance(available_levels, list):
            available_levels = [available_levels]

        # æ£€æŸ¥ç”¨æˆ·æŒ‡å®šçš„ levels æ˜¯å¦åœ¨å¯ç”¨çš„ levels ä¸­
        invalid_levels = [l for l in selected_levels if l not in available_levels]
        if invalid_levels:
            raise ValueError(
                f"Invalid levels: {invalid_levels}. "
                f"Available levels from config: {available_levels}"
            )

        # å¯¹äºå•levelè®­ç»ƒï¼Œæ‰€æœ‰é€šé“éƒ½æ¥è‡ªåŒä¸€levelï¼Œæ— éœ€ç‰¹æ®Šå¤„ç†
        print(f"  é¢„æµ‹æ•°æ®å°†ä½¿ç”¨levels: {selected_levels}")
    else:
        print(f"  é¢„æµ‹æ•°æ®å°†ä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ—¶çš„levels")
        selected_levels = available_levels

    # åŠ è½½å½’ä¸€åŒ–å‚æ•°
    normalizer_path = model_dir / "normalizer_stats.pkl"
    with open(normalizer_path, "rb") as f:
        normalizer_data = pickle.load(f)

    print(f"âœ“ åŠ è½½å½’ä¸€åŒ–å‚æ•°: {normalizer_path}")

    # ========================================================================
    # Step 2: åŠ è½½æ•°æ®ï¼ˆé¢„æµ‹æ¨¡å¼ï¼šä¸åˆ†å‰²æ•°æ®ï¼‰
    # ========================================================================
    print("\n" + "-" * 80)
    print("Step 2: åŠ è½½æ•°æ®")
    print("-" * 80)

    # ç›´æ¥åŠ è½½æ•°æ®ï¼Œä¸ä½¿ç”¨WeatherDataModuleçš„åˆ†å‰²é€»è¾‘
    import xarray as xr
    from torch.utils.data import DataLoader
    from weatherdiff.utils import (
        prepare_weather_data,
        WeatherSequenceDataset,
        Normalizer,
    )

    print(f"åŠ è½½æ•°æ®: {args.data_path}")
    ds = xr.open_zarr(args.data_path)

    # æ—¶é—´åˆ‡ç‰‡
    start, end = args.time_slice.split(":")
    ds = ds.sel(time=slice(start, end))

    # è·å–å˜é‡æ•°æ®
    variable_da = ds[config["variables"]]

    # å¦‚æœæœ‰levelç»´åº¦ï¼Œæ ¹æ®configä¸­çš„levelsè¿›è¡Œé€‰æ‹©ï¼ˆç”¨äºæ•°æ®åŠ è½½ï¼‰
    # æ³¨æ„ï¼šæ•°æ®åŠ è½½æ—¶éœ€è¦ä½¿ç”¨è®­ç»ƒæ—¶ä½¿ç”¨çš„æ‰€æœ‰levelsï¼Œè€Œä¸æ˜¯å‘½ä»¤è¡ŒæŒ‡å®šçš„selected_levels
    # selected_levelsåªç”¨äºåç»­çš„è¯„ä¼°å’Œå¯è§†åŒ–
    if "level" in variable_da.dims:
        if available_levels is not None:
            # ä½¿ç”¨è®­ç»ƒæ—¶æŒ‡å®šçš„ levelsï¼ˆæ‰€æœ‰levelsï¼‰
            print(f"  å˜é‡æœ‰levelç»´åº¦ï¼Œä½¿ç”¨è®­ç»ƒæ—¶çš„levels: {available_levels}")
            variable_da = variable_da.sel(level=available_levels)
            # ç¡®ä¿é¡ºåºä¸è®­ç»ƒæ—¶ä¸€è‡´
            actual_levels = variable_da.level.values.tolist()
            print(f"  å®é™…åŠ è½½çš„levels: {actual_levels}")
        else:
            # ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ levels
            available_levels_from_data = variable_da.level.values.tolist()
            print(
                f"  å˜é‡æœ‰levelç»´åº¦ï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„levels: {available_levels_from_data}"
            )
            available_levels = available_levels_from_data
            actual_levels = available_levels_from_data

    # ç»Ÿä¸€çº¬ç»åº¦ç»´åº¦é¡ºåºï¼Œä¸è®­ç»ƒ/WeatherDataLoaderä¿æŒä¸€è‡´
    if "latitude" in variable_da.dims and "longitude" in variable_da.dims:
        dims = list(variable_da.dims)
        lon_idx = dims.index("longitude")
        lat_idx = dims.index("latitude")

        if lon_idx < lat_idx:
            print(f"  æ£€æµ‹åˆ°ç»´åº¦é¡ºåº {dims} (longitude åœ¨ latitude å‰)ï¼Œè½¬ç½®ä¸ºæ ‡å‡†é¡ºåº")
            if "level" in dims:
                target_dims = ["time", "level", "latitude", "longitude"]
            else:
                target_dims = ["time", "latitude", "longitude"]
            variable_da = variable_da.transpose(*target_dims)
            print(f"  è½¬ç½®åç»´åº¦: {variable_da.dims}")
        else:
            print(f"  ç»´åº¦é¡ºåºç¬¦åˆæ ‡å‡†: {dims}")
    else:
        print(f"  æœªæ£€æµ‹åˆ°çº¬/ç»åº¦ç»´åº¦ï¼Œdims={variable_da.dims}")

    lon_first_flag = detect_lon_first(variable_da)
    if lon_first_flag is not None:
        config["_spatial_lon_first"] = lon_first_flag
        orientation_desc = (
            "Longitude-First (lon->lat)"
            if lon_first_flag
            else "Latitude-First (lat->lon)"
        )
        print(f"  ç©ºé—´ç»´åº¦é¡ºåº: {orientation_desc} | dims={variable_da.dims}")
    else:
        print(f"  ç©ºé—´ç»´åº¦é¡ºåº: æœªæ£€æµ‹åˆ°çº¬/ç»åº¦åç§° | dims={variable_da.dims}")

    data = variable_da.values  # (Time, H, W) æˆ– (Time, Level, H, W)
    print(f"åŸå§‹æ•°æ® shape: {data.shape}")
    print(f"æ•°æ®èŒƒå›´: [{data.min():.2f}, {data.max():.2f}]")
    print(f"æ—¶é—´èŒƒå›´: {start} è‡³ {end}")

    # è·å–target_sizeï¼ˆå¦‚æœæœ‰ï¼‰
    target_size = None
    if "target_size" in config and config["target_size"]:
        if isinstance(config["target_size"], str):
            target_size = tuple(map(int, config["target_size"].split(",")))
        else:
            target_size = tuple(config["target_size"])

    # å‡†å¤‡ä¸ºå›¾åƒæ ¼å¼
    data = prepare_weather_data(
        data, n_channels=config["n_channels"], target_size=target_size
    )
    print(f"å¤„ç†å shape: {data.shape}")
    if target_size:
        print(f"  å›¾åƒå°ºå¯¸: {target_size}")
        # éªŒè¯æ•°æ®å°ºå¯¸æ˜¯å¦ä¸target_sizeä¸€è‡´
        _, _, data_H, data_W = data.shape
        if data_H != target_size[0] or data_W != target_size[1]:
            raise ValueError(
                f"æ•°æ®å°ºå¯¸ä¸åŒ¹é…ï¼š\n"
                f"  æ•°æ®åŠ è½½åçš„å°ºå¯¸: ({data_H}, {data_W})\n"
                f"  è®­ç»ƒæ—¶target_size: {target_size}\n"
                f"  å¯èƒ½åŸå› ï¼šprepare_weather_dataæœªæ­£ç¡®resizeåˆ°target_size\n"
                f"  è§£å†³æ–¹æ¡ˆï¼šæ£€æŸ¥prepare_weather_dataçš„target_sizeå‚æ•°æ˜¯å¦æ­£ç¡®ä¼ é€’"
            )

    # å½’ä¸€åŒ–ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„å‚æ•°ï¼‰
    normalizer = Normalizer(method=config["normalization"])

    # åŠ è½½å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆæ”¯æŒæŒ‰levelå½’ä¸€åŒ–ï¼‰
    if (
        "normalize_per_level" in normalizer_data
        and normalizer_data["normalize_per_level"]
    ):
        # æŒ‰levelå½’ä¸€åŒ–ï¼šä¼ é€’å®Œæ•´çš„normalizer_data
        normalizer.load_stats(normalizer_data)
        # ä»configæˆ–metadataä¸­è·å–n_channels_per_level
        if "n_channels" in config:
            normalizer.n_channels_per_level = config["n_channels"]
        else:
            # å°è¯•ä»æ•°æ®å½¢çŠ¶æ¨æ–­
            _, C, _, _ = data.shape
            if "levels" in normalizer_data and normalizer_data["levels"]:
                n_levels = len(normalizer_data["levels"])
                if C % n_levels == 0:
                    normalizer.n_channels_per_level = C // n_levels
                    print(f"  æ¨æ–­æ¯ä¸ªlevelçš„é€šé“æ•°: {normalizer.n_channels_per_level}")
                else:
                    raise ValueError(
                        f"æ— æ³•æ¨æ–­æ¯ä¸ªlevelçš„é€šé“æ•°ã€‚æ€»é€šé“æ•°: {C}, Levelsæ•°: {n_levels}"
                    )
    else:
        # å…¨å±€å½’ä¸€åŒ–ï¼šåªä¼ é€’stats
        normalizer.load_stats(normalizer_data.get("stats", normalizer_data))

    data = normalizer.transform(data, name=config["variables"])
    print(f"å½’ä¸€åŒ–åèŒƒå›´: [{data.min():.2f}, {data.max():.2f}]")

    # åˆ›å»ºå®Œæ•´çš„åºåˆ—æ•°æ®é›†ï¼ˆä¸åˆ†å‰²ï¼‰
    full_dataset = WeatherSequenceDataset(
        data, config["input_length"], config["output_length"]
    )

    test_loader = DataLoader(
        full_dataset,
        batch_size=args.batch_size,
        shuffle=False,
        num_workers=4,
        pin_memory=True,
    )

    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆï¼ˆé¢„æµ‹æ¨¡å¼ï¼šä¸åˆ†å‰²ï¼‰")
    print(f"  æ€»æ ·æœ¬æ•°: {len(full_dataset)}")
    print(f"  æ‰¹æ¬¡æ•°: {len(test_loader)}")

    # ========================================================================
    # Step 3: åŠ è½½æ¨¡å‹
    # ========================================================================
    print("\n" + "-" * 80)
    print("Step 3: åŠ è½½æ¨¡å‹")
    print("-" * 80)

    # è·å–æ•°æ®å½¢çŠ¶ä¿¡æ¯
    sample_input, sample_output = full_dataset[0]
    T_in, C, H, W = sample_input.shape
    T_out = sample_output.shape[0]

    in_channels = T_in * C
    out_channels = T_out * C

    model = WeatherUNet(
        in_channels=in_channels,
        out_channels=out_channels,
        base_channels=config["base_channels"],
        depth=config["depth"],
    )

    checkpoint_path = model_dir / "best_model.pt"
    checkpoint = torch.load(checkpoint_path, map_location=args.device)
    model.load_state_dict(checkpoint["model_state_dict"])
    model = model.to(args.device)
    model.eval()

    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {checkpoint_path}")
    print(f"  è®­ç»ƒepoch: {checkpoint['epoch']}")
    print(f"  éªŒè¯æŸå¤±: {checkpoint['val_loss']:.6f}")
    print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # ========================================================================
    # Step 4: é¢„æµ‹
    # ========================================================================
    print("\n" + "-" * 80)
    print("Step 4: ç”Ÿæˆé¢„æµ‹")
    print("-" * 80)

    all_predictions = []
    all_targets = []
    all_inputs = []

    with torch.no_grad():
        for inputs, targets in tqdm(test_loader, desc="é¢„æµ‹ä¸­"):
            inputs = inputs.to(args.device)
            B, T_in, C, H, W = inputs.shape
            T_out = targets.shape[1]

            # å±•å¹³æ—¶é—´ç»´åº¦
            inputs_flat = inputs.reshape(B, T_in * C, H, W)
            outputs = model(inputs_flat)
            outputs = outputs.reshape(B, T_out, C, H, W)

            all_predictions.append(outputs.cpu().numpy())
            all_targets.append(targets.numpy())
            all_inputs.append(inputs.cpu().numpy())

    y_pred = np.concatenate(all_predictions, axis=0)
    y_true = np.concatenate(all_targets, axis=0)
    X = np.concatenate(all_inputs, axis=0)

    print(f"âœ“ é¢„æµ‹å®Œæˆ")
    print(f"  è¾“å…¥å½¢çŠ¶: {X.shape}")
    print(f"  é¢„æµ‹å½¢çŠ¶: {y_pred.shape}")
    print(f"  çœŸå€¼å½¢çŠ¶: {y_true.shape}")

    return (
        y_pred,
        y_true,
        normalizer,
        config,
        output_dir,
        selected_levels,
    )


def main():
    parser = argparse.ArgumentParser(description="åƒç´ ç©ºé—´U-Neté¢„æµ‹è„šæœ¬")

    # æ¨¡å‹å‚æ•°
    parser.add_argument(
        "--model-dir",
        type=str,
        required=True,
        help="æ¨¡å‹ç›®å½•ï¼ˆåŒ…å«best_model.ptå’Œconfig.jsonï¼‰",
    )

    # æ•°æ®å‚æ•°
    parser.add_argument(
        "--data-path",
        type=str,
        default="gs://weatherbench2/datasets/era5/1959-2022-6h-64x32_equiangular_conservative.zarr",
        help="æ•°æ®è·¯å¾„",
    )
    parser.add_argument(
        "--time-slice", type=str, default="2020-01-01:2020-12-31", help="é¢„æµ‹æ—¶é—´èŒƒå›´"
    )

    # è¾“å‡ºå‚æ•°
    parser.add_argument(
        "--output-dir", type=str, default=None, help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä½¿ç”¨æ¨¡å‹ç›®å½•ï¼‰"
    )
    parser.add_argument("--batch-size", type=int, default=32, help="é¢„æµ‹æ‰¹æ¬¡å¤§å°")
    parser.add_argument(
        "--levels",
        type=int,
        nargs="+",
        default=None,
        help="é€‰æ‹©ç‰¹å®šçš„æ°”å‹å±‚è¿›è¡Œè¯„ä¼°/å¯è§†åŒ– (e.g. --levels 500 or --levels 500 700 850). "
        "Levelså¿…é¡»ä¸è®­ç»ƒæ—¶é…ç½®ä¸­çš„levelsä¸€è‡´ã€‚å¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨æ‰€æœ‰levelsã€‚",
    )

    # å…¶ä»–å‚æ•°
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="è®¾å¤‡",
    )

    args = parser.parse_args()

    # ========================================================================
    # åƒç´ ç©ºé—´U-Neté¢„æµ‹
    # ========================================================================
    (
        y_pred,
        y_true,
        normalizer,
        config,
        output_dir,
        selected_levels,
    ) = predict_pixel_unet(args)

    # ========================================================================
    # Step 6: åå½’ä¸€åŒ–
    # ========================================================================
    print("\n" + "-" * 80)
    print("Step 6: è¯„ä¼°å’Œåå½’ä¸€åŒ–")
    print("-" * 80)

    # å¯¹äºå•levelè®­ç»ƒçš„æ¨¡å‹ï¼Œä½¿ç”¨æ‰€æœ‰é€šé“ï¼ˆéƒ½æ¥è‡ªåŒä¸€levelï¼‰
    y_pred_selected = y_pred
    y_true_selected = y_true

    # å½’ä¸€åŒ–ç©ºé—´çš„æŒ‡æ ‡
    print("\nå½’ä¸€åŒ–ç©ºé—´çš„æŒ‡æ ‡:")
    metrics_norm = calculate_metrics(y_pred_selected, y_true_selected, ensemble=False)

    # æ·»åŠ MSEè®¡ç®—ï¼ˆå‚è€ƒpredict.pyï¼‰
    mse_norm = np.mean((y_pred_selected - y_true_selected) ** 2)
    metrics_norm["mse"] = float(mse_norm)

    print(format_metrics(metrics_norm))

    # åå½’ä¸€åŒ–åˆ°ç‰©ç†å•ä½
    variable = config["variables"]
    C = y_pred.shape[2]
    H = y_pred.shape[3]
    W = y_pred.shape[4]

    y_pred_flat = y_pred.reshape(-1, C, H, W)
    y_true_flat = y_true.reshape(-1, C, H, W)

    y_pred_phys = normalizer.inverse_transform(y_pred_flat, name=variable)
    y_true_phys = normalizer.inverse_transform(y_true_flat, name=variable)

    y_pred_phys = y_pred_phys.reshape(y_pred.shape)
    y_true_phys = y_true_phys.reshape(y_true.shape)

    print("\nâœ“ åå½’ä¸€åŒ–å®Œæˆ")
    print(f"  é¢„æµ‹èŒƒå›´: [{y_pred_phys.min():.2f}, {y_pred_phys.max():.2f}] K")
    print(f"  çœŸå€¼èŒƒå›´: [{y_true_phys.min():.2f}, {y_true_phys.max():.2f}] K")

    # å¯¹äºå•levelè®­ç»ƒçš„æ¨¡å‹ï¼Œä½¿ç”¨æ‰€æœ‰é€šé“ï¼ˆéƒ½æ¥è‡ªåŒä¸€levelï¼‰
    y_pred_phys_selected = y_pred_phys
    y_true_phys_selected = y_true_phys

    # ç‰©ç†ç©ºé—´çš„æŒ‡æ ‡
    print("\nç‰©ç†ç©ºé—´çš„æŒ‡æ ‡ (åŸå§‹å°ºåº¦):")
    metrics_phys = calculate_metrics(
        y_pred_phys_selected, y_true_phys_selected, ensemble=False
    )

    # æ·»åŠ MSEè®¡ç®—ï¼ˆå‚è€ƒpredict.pyï¼‰
    mse_phys = np.mean((y_pred_phys_selected - y_true_phys_selected) ** 2)
    metrics_phys["mse"] = float(mse_phys)

    print(format_metrics(metrics_phys))

    # è®¡ç®—æ¯ä¸ªlead timeçš„RMSEå’ŒMSE
    print("\næ¯ä¸ªlead timeçš„RMSEå’ŒMSE:")
    T_out = y_pred_phys_selected.shape[1]
    rmse_per_leadtime = {}
    mse_per_leadtime = {}
    for t in range(T_out):
        y_pred_t = y_pred_phys_selected[:, t, :, :, :]  # (N, C, H, W)
        y_true_t = y_true_phys_selected[:, t, :, :, :]
        mse_t = np.mean((y_pred_t - y_true_t) ** 2)
        rmse_t = np.sqrt(mse_t)
        rmse_per_leadtime[f"rmse_step_{t+1}"] = float(rmse_t)
        mse_per_leadtime[f"mse_step_{t+1}"] = float(mse_t)
        print(f"  Step {t+1} ({(t+1)*6}h): RMSE = {rmse_t:.4f} K, MSE = {mse_t:.4f} KÂ²")

    # ========================================================================
    # Step 7: ä¿å­˜ç»“æœ
    # ========================================================================
    print("\n" + "-" * 80)
    print("Step 7: ä¿å­˜ç»“æœ")
    print("-" * 80)

    # ä¿å­˜æŒ‡æ ‡
    metrics_all = {
        "mode": "pixel",
        "normalized_space": {k: float(v) for k, v in metrics_norm.items()},
        "physical_space": {k: float(v) for k, v in metrics_phys.items()},
        "physical_space_rmse_per_leadtime": rmse_per_leadtime,
        "physical_space_mse_per_leadtime": mse_per_leadtime,
        "time_slice": args.time_slice,
        "n_samples": int(y_pred.shape[0]),
    }

    metrics_path = output_dir / "prediction_metrics.json"
    with open(metrics_path, "w") as f:
        json.dump(metrics_all, f, indent=2)
    print(f"âœ“ æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")

    # ä¿å­˜é¢„æµ‹æ•°æ®
    pred_dir = output_dir / "predictions_data"
    pred_dir.mkdir(exist_ok=True)
    np.save(pred_dir / "y_test_pred_norm.npy", y_pred)
    np.save(pred_dir / "y_test_norm.npy", y_true)
    np.save(pred_dir / "y_test.npy", y_true_phys)  # çœŸå€¼
    np.save(pred_dir / "y_test_pred.npy", y_pred_phys)  # é¢„æµ‹å€¼
    print(f"âœ“ é¢„æµ‹æ•°æ®å·²ä¿å­˜: {pred_dir}/y_*.npy")

    # ========================================================================
    # Step 8: ç”Ÿæˆå¯è§†åŒ–
    # ========================================================================
    print("\n" + "-" * 80)
    print("Step 8: ç”Ÿæˆå¯è§†åŒ–")
    print("-" * 80)

    # è·å–ç©ºé—´åæ ‡
    import xarray as xr

    ds = xr.open_zarr(args.data_path)

    spatial_coords = None
    if hasattr(ds, "latitude") and hasattr(ds, "longitude"):
        lat_values = ds.latitude.values
        lon_values = ds.longitude.values

        # è·å–é¢„æµ‹æ•°æ®çš„å®é™…ç©ºé—´å½¢çŠ¶
        # y_pred_phys shape: (N, T, C, H, W)
        actual_H = y_pred_phys.shape[3]
        actual_W = y_pred_phys.shape[4]

        print(f"\næ£€æŸ¥ç©ºé—´åæ ‡:")
        print(f"  æ•°æ®é›†åæ ‡: lat={len(lat_values)}, lon={len(lon_values)}")
        print(f"  é¢„æµ‹æ•°æ®å½¢çŠ¶: H={actual_H}, W={actual_W}")

        # éªŒè¯åæ ‡é•¿åº¦æ˜¯å¦åŒ¹é…ï¼ˆç®€å•ç›´æ¥çš„æ–¹å¼ï¼Œå‚è€ƒpredict.pyï¼‰
        if len(lat_values) == actual_H and len(lon_values) == actual_W:
            spatial_coords = {
                "lat": lat_values,
                "lon": lon_values,
            }
            print("  âœ“ åæ ‡åŒ¹é…ï¼Œä½¿ç”¨æ•°æ®é›†åæ ‡")
        else:
            print(f"  âš  åæ ‡ç»´åº¦ä¸åŒ¹é…")
            print(f"  ä½¿ç”¨é»˜è®¤åæ ‡ç”Ÿæˆå¯è§†åŒ–...")
            spatial_coords = {
                "lat": np.linspace(-90, 90, actual_H),
                "lon": np.linspace(0, 360, actual_W),
            }
    else:
        # é»˜è®¤åæ ‡
        actual_H = y_pred_phys.shape[3]
        actual_W = y_pred_phys.shape[4]
        print("  æ•°æ®é›†æ²¡æœ‰åæ ‡ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤åæ ‡")
        spatial_coords = {
            "lat": np.linspace(-90, 90, actual_H),
            "lon": np.linspace(0, 360, actual_W),
        }

    # ç”Ÿæˆå¯è§†åŒ–
    # æ³¨æ„ï¼šWeatherDiffä½¿ç”¨minmaxå½’ä¸€åŒ–ï¼ˆ[-1,1]ï¼‰ï¼Œä¸ä¼ ç»Ÿæ¨¡å‹çš„zscoreä¸åŒ
    # è¿™é‡Œä¼ é€’å·²ç»åå½’ä¸€åŒ–çš„ç‰©ç†å€¼æ•°æ®ï¼Œnorm_params=None
    # å¯è§†åŒ–å‡½æ•°ä¼šç”Ÿæˆtimeseries_overallï¼ˆç‰©ç†å€¼ï¼‰ï¼Œä½†ä¸ä¼šç”Ÿæˆtimeseries_physical
    # ï¼ˆå› ä¸ºWeatherDiffçš„å½’ä¸€åŒ–æ–¹å¼ä¸å¯è§†åŒ–å‡½æ•°é¢„æœŸä¸å…¼å®¹ï¼‰
    visualize_predictions_improved(
        y_true_phys_selected,  # y_test (ç‰©ç†å€¼æ•°æ®ï¼Œå¯èƒ½å·²è½¬ç½®)
        y_pred_phys_selected,  # y_test_pred (ç‰©ç†å€¼æ•°æ®ï¼Œå¯èƒ½å·²è½¬ç½®)
        metrics_phys,  # test_metrics (ç‰©ç†ç©ºé—´æŒ‡æ ‡)
        [variable],  # variables
        "pixel_unet",  # model_name
        output_dir,  # output_dir
        "spatial",  # data_format
        norm_params=None,  # norm_params (ä¸æä¾›ï¼Œå› ä¸ºå½’ä¸€åŒ–æ–¹å¼ä¸åŒ)
        spatial_coords=spatial_coords,  # spatial_coords
    )

    print(f"âœ“ å¯è§†åŒ–å·²ç”Ÿæˆ")
    print(f"  - timeseries_overall_{variable}.png (ç‰©ç†å€¼)")
    print(
        f"  - timeseries_physical_{variable}.png (æœªç”Ÿæˆï¼Œå› ä¸ºå·²ä½¿ç”¨ç‰©ç†å€¼ç»˜åˆ¶overall)"
    )
    print(f"  - leadtime_independent_{variable}.png")
    print(f"  - rmse_vs_leadtime_{variable}.png")
    print(f"  - spatial_comparison_{variable}.png")

    # ========================================================================
    # æ€»ç»“
    # ========================================================================
    print("\n" + "=" * 80)
    print("é¢„æµ‹å®Œæˆ!")
    print("=" * 80)

    print(f"\næ¨¡å¼: PIXEL")
    print(f"æ€»ç»“ (ç‰©ç†ç©ºé—´):")
    print(f"  æ ·æœ¬æ•°: {y_pred.shape[0]}")
    print(f"  MSE: {metrics_phys['mse']:.4f} KÂ²")
    print(f"  RMSE: {metrics_phys['rmse']:.4f} K")
    print(f"  MAE: {metrics_phys['mae']:.4f} K")
    print(f"  ç›¸å…³ç³»æ•°: {metrics_phys['correlation']:.4f}")
    print(f"  SSIM: {metrics_phys['ssim']:.4f}")

    print(f"\nç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"  - prediction_metrics.json: è¯¦ç»†æŒ‡æ ‡")
    print(f"  - y_pred_*.npy: é¢„æµ‹æ•°æ®")
    print(f"  - *.png: å¯è§†åŒ–å›¾ç‰‡")

    # æ€§èƒ½è¯„ä»·
    if metrics_phys["rmse"] < 3.0:
        print("\nâœ… é¢„æµ‹æ•ˆæœä¼˜ç§€ï¼")
    elif metrics_phys["rmse"] < 5.0:
        print("\nâœ… é¢„æµ‹æ•ˆæœè‰¯å¥½ï¼")
    elif metrics_phys["rmse"] < 10.0:
        print("\nâš ï¸  é¢„æµ‹æ•ˆæœä¸€èˆ¬")
    else:
        print("\nâš ï¸  é¢„æµ‹æ•ˆæœè¾ƒå·®ï¼Œå»ºè®®:")
        print("  1. å¢åŠ è®­ç»ƒæ•°æ®é‡")
        print("  2. å¢åŠ è®­ç»ƒè½®æ•°")
        print("  3. è°ƒæ•´æ¨¡å‹å‚æ•°")


if __name__ == "__main__":
    main()
