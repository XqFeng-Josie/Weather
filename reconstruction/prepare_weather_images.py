#!/usr/bin/env python3
"""
å°† ERA5 å¤©æ°”æ•°æ®ï¼ˆ64Ã—32ï¼‰æ’å€¼åˆ°ç›®æ ‡å°ºå¯¸ï¼ˆå¦‚ 256Ã—256ï¼‰å¹¶ä¿å­˜ä¸ºå›¾ç‰‡

ä½¿ç”¨æ–¹æ³•:
    python prepare_weather_images.py \
        --data-path gs://weatherbench2/datasets/era5/1959-2022-6h-64x32_equiangular_conservative.zarr \
        --variable 2m_temperature \
        --time-slice 2020-01-01:2020-01-31 \
        --target-size 256 256 \
        --output-dir weather_images \
        --n-samples 100
"""

import argparse
import json
import os
import sys
import time
import warnings
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from functools import partial
from pathlib import Path

import numpy as np
import xarray as xr
from PIL import Image
from scipy.ndimage import zoom
from tqdm import tqdm

PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

warnings.filterwarnings("ignore")


def _compute_chunk_stats(args_tuple):
    """
    è®¡ç®—æ•°æ®å—çš„ç»Ÿè®¡é‡ï¼ˆç”¨äºå¹¶å‘è®¡ç®—ï¼‰

    Args:
        args_tuple: (chunk_data, method)

    Returns:
        dict with stats
    """
    chunk_data, method = args_tuple
    stats = {}

    if method == "minmax":
        stats["min"] = float(chunk_data.min())
        stats["max"] = float(chunk_data.max())
    elif method == "zscore":
        stats["mean"] = float(chunk_data.mean())
        stats["sum"] = float(chunk_data.sum())
        stats["sum_sq"] = float((chunk_data**2).sum())
        stats["count"] = int(chunk_data.size)

    return stats


def _normalize_chunk(args_tuple):
    """
    å½’ä¸€åŒ–æ•°æ®å—ï¼ˆç”¨äºå¹¶å‘å¤„ç†ï¼‰

    Args:
        args_tuple: (chunk_data, method, stats)

    Returns:
        normalized_chunk
    """
    chunk_data, method, stats = args_tuple

    if method == "minmax":
        global_min = stats["min"]
        global_max = stats["max"]
        denom = (global_max - global_min) + 1e-8
        normalized = (chunk_data - global_min) / denom
        normalized = np.clip(normalized * 255.0, 0, 255).astype(np.uint8)
    elif method == "zscore":
        global_mean = stats["mean"]
        global_std = stats["std"]
        normalized = (chunk_data - global_mean) / (global_std + 1e-8)
        normalized = np.clip((normalized + 3) / 6 * 255.0, 0, 255).astype(np.uint8)
    else:
        raise ValueError(f"Unknown normalization method: {method}")

    return normalized


def _process_single_time_step(args_tuple):
    """
    å¤„ç†å•ä¸ªæ—¶é—´æ­¥çš„è¾…åŠ©å‡½æ•°ï¼ˆç”¨äºå¹¶å‘å¤„ç†ï¼‰

    Args:
        args_tuple: (time_slice, n_channels, target_size, idx)

    Returns:
        (idx, processed_slice)
    """
    time_slice, n_channels, target_size, idx = args_tuple

    # ç¡®ä¿æ˜¯3ç»´: (C, H, W) æˆ– (H, W)
    if time_slice.ndim == 2:
        time_slice = time_slice[np.newaxis, :, :]  # (1, H, W)

    # å¦‚æœéœ€è¦3é€šé“ä½†åªæœ‰1é€šé“ï¼Œå¤åˆ¶
    if n_channels == 3 and time_slice.shape[0] == 1:
        time_slice = np.repeat(time_slice, 3, axis=0)

    # è°ƒæ•´å°ºå¯¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if target_size is not None:
        c, h, w = time_slice.shape
        target_h, target_w = target_size

        zoom_factors = (1, target_h / h, target_w / w)
        time_slice = zoom(time_slice, zoom_factors, order=3)  # order=3 åŒä¸‰æ¬¡æ’å€¼

    return (idx, time_slice)


def prepare_weather_data(
    data: np.ndarray,
    n_channels: int = 3,
    target_size: tuple = None,
    n_workers: int = 1,
    use_concurrent: bool = False,
) -> np.ndarray:
    """
    å‡†å¤‡å¤©æ°”æ•°æ®ç”¨äºå›¾åƒæ¨¡å‹

    Args:
        data: è¾“å…¥æ•°æ®ï¼Œshape (Time, H, W) æˆ– (Time, C, H, W)
        n_channels: ç›®æ ‡é€šé“æ•°ï¼ˆ1æˆ–3ï¼‰
        target_size: ç›®æ ‡å°ºå¯¸ (H, W)ï¼Œå¦‚æœä¸ºNoneåˆ™ä¿æŒåŸå°ºå¯¸
        n_workers: å¹¶å‘å·¥ä½œè¿›ç¨‹æ•°ï¼ˆä»…åœ¨use_concurrent=Trueæ—¶æœ‰æ•ˆï¼‰
        use_concurrent: æ˜¯å¦ä½¿ç”¨å¹¶å‘å¤„ç†ï¼ˆå¯¹äºå¤§æ•°æ®é›†å¯èƒ½æ›´å¿«ï¼‰

    Returns:
        å¤„ç†åçš„æ•°æ®ï¼Œshape (Time, C, H, W)
    """
    # ç¡®ä¿æ˜¯4ç»´: (Time, C, H, W)
    if data.ndim == 3:
        # (Time, H, W) -> (Time, 1, H, W)
        data = data[:, np.newaxis, :, :]

    # å¦‚æœéœ€è¦3é€šé“ä½†åªæœ‰1é€šé“ï¼Œå¤åˆ¶
    if n_channels == 3 and data.shape[1] == 1:
        data = np.repeat(data, 3, axis=1)

    # è°ƒæ•´å°ºå¯¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if target_size is not None:
        time, c, h, w = data.shape
        target_h, target_w = target_size

        # å¯¹äºå¤§æ•°æ®é›†ï¼Œå¯ä»¥è€ƒè™‘å¹¶å‘å¤„ç†æ¯ä¸ªæ—¶é—´æ­¥
        # æ³¨æ„ï¼šä½¿ç”¨ThreadPoolExecutorè€Œä¸æ˜¯ProcessPoolExecutorï¼Œå› ä¸ºscipyçš„zoomæ“ä½œ
        # ä¸»è¦åœ¨Cæ‰©å±•ä¸­æ‰§è¡Œï¼ŒGILå½±å“è¾ƒå°ï¼Œä¸”é¿å…åºåˆ—åŒ–å¼€é”€
        if use_concurrent and n_workers > 1 and time > 10:
            # å‡†å¤‡ä»»åŠ¡
            tasks = []
            for i in range(time):
                tasks.append((data[i].copy(), n_channels, target_size, i))

            # å¹¶å‘å¤„ç†
            results = {}
            failed_count = 0
            with ThreadPoolExecutor(max_workers=n_workers) as executor:
                futures = {
                    executor.submit(_process_single_time_step, task): task[3]
                    for task in tasks
                }

                with tqdm(
                    total=time,
                    desc="ğŸ”„ æ’å€¼å¤„ç†",
                    unit="å¸§",
                    unit_scale=False,
                    bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
                ) as pbar:
                    for future in as_completed(futures):
                        try:
                            idx, processed = future.result()
                            results[idx] = processed
                        except Exception as e:
                            failed_count += 1
                            task_idx = futures[future]
                            print(f"\nâš ï¸  å¤„ç†å¸§ {task_idx} æ—¶å‡ºé”™: {e}")
                        pbar.update(1)
                        pbar.set_postfix({"å¤±è´¥": failed_count})

            if failed_count > 0:
                print(f"\nâš ï¸  è­¦å‘Š: {failed_count} å¸§å¤„ç†å¤±è´¥")

            # æŒ‰é¡ºåºé‡ç»„æ•°æ®
            processed_data = np.stack([results[i] for i in range(time)], axis=0)
            return processed_data
        else:
            # åŸå§‹æ–¹æ³•ï¼šä¸€æ¬¡æ€§å¤„ç†æ‰€æœ‰æ—¶é—´æ­¥
            zoom_factors = (1, 1, target_h / h, target_w / w)
            # data = zoom(data, zoom_factors, order=1)  # order=1 è¡¨ç¤ºåŒçº¿æ€§æ’å€¼
            data = zoom(data, zoom_factors, order=3)  # order=3 åŒä¸‰æ¬¡æ’å€¼

    return data


def compute_normalization_stats_concurrent(
    data: np.ndarray,
    method: str = "minmax",
    n_workers: int = 4,
    chunk_size: int = None,
) -> dict:
    """
    å¹¶å‘è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡

    Args:
        data: è¾“å…¥æ•°æ®
        method: å½’ä¸€åŒ–æ–¹æ³•
        n_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
        chunk_size: æ¯ä¸ªå—çš„å¤§å°ï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨è®¡ç®—ï¼‰

    Returns:
        ç»Ÿè®¡é‡å­—å…¸
    """
    if chunk_size is None:
        # è‡ªåŠ¨è®¡ç®—åˆé€‚çš„å—å¤§å°
        total_elements = data.size
        chunk_size = max(1, total_elements // (n_workers * 4))

    if method == "minmax":
        # å°†æ•°æ®åˆ†æˆå—
        data_flat = data.flatten()
        n_chunks = (len(data_flat) + chunk_size - 1) // chunk_size
        chunks = [
            data_flat[i * chunk_size : (i + 1) * chunk_size] for i in range(n_chunks)
        ]

        # å¹¶å‘è®¡ç®—æ¯ä¸ªå—çš„æœ€å°å€¼å’Œæœ€å¤§å€¼
        tasks = [(chunk, method) for chunk in chunks]

        with ThreadPoolExecutor(max_workers=n_workers) as executor:
            chunk_stats = list(
                tqdm(
                    executor.map(_compute_chunk_stats, tasks),
                    total=len(tasks),
                    desc="ğŸ“Š è®¡ç®—ç»Ÿè®¡é‡",
                    unit="å—",
                    leave=False,
                )
            )

        # åˆå¹¶ç»“æœ
        global_min = min(stat["min"] for stat in chunk_stats)
        global_max = max(stat["max"] for stat in chunk_stats)

        return {"min": global_min, "max": global_max}

    elif method == "zscore":
        # å°†æ•°æ®åˆ†æˆå—
        data_flat = data.flatten()
        n_chunks = (len(data_flat) + chunk_size - 1) // chunk_size
        chunks = [
            data_flat[i * chunk_size : (i + 1) * chunk_size] for i in range(n_chunks)
        ]

        # å¹¶å‘è®¡ç®—æ¯ä¸ªå—çš„ç»Ÿè®¡é‡
        tasks = [(chunk, method) for chunk in chunks]

        with ThreadPoolExecutor(max_workers=n_workers) as executor:
            chunk_stats = list(
                tqdm(
                    executor.map(_compute_chunk_stats, tasks),
                    total=len(tasks),
                    desc="ğŸ“Š è®¡ç®—ç»Ÿè®¡é‡",
                    unit="å—",
                    leave=False,
                )
            )

        # åˆå¹¶ç»“æœï¼ˆè®¡ç®—å…¨å±€å‡å€¼å’Œæ ‡å‡†å·®ï¼‰
        total_sum = sum(stat["sum"] for stat in chunk_stats)
        total_sum_sq = sum(stat["sum_sq"] for stat in chunk_stats)
        total_count = sum(stat["count"] for stat in chunk_stats)

        global_mean = total_sum / total_count
        global_var = (total_sum_sq / total_count) - (global_mean**2)
        global_std = np.sqrt(max(0, global_var))

        return {"mean": global_mean, "std": global_std}

    else:
        raise ValueError(f"Unknown normalization method: {method}")


def normalize_to_image(data: np.ndarray, method: str = "minmax") -> np.ndarray:
    """
    å°†æ•°æ®å½’ä¸€åŒ–åˆ° [0, 255] èŒƒå›´ä»¥ä¾¿ä¿å­˜ä¸ºå›¾ç‰‡

    Args:
        data: è¾“å…¥æ•°æ®ï¼Œshape (Time, C, H, W) æˆ– (H, W)
        method: å½’ä¸€åŒ–æ–¹æ³• ('minmax' æˆ– 'zscore')

    Returns:
        å½’ä¸€åŒ–åçš„æ•°æ®ï¼ŒèŒƒå›´ [0, 255]ï¼Œdtype uint8
    """
    if method == "minmax":
        # MinMax å½’ä¸€åŒ–åˆ° [0, 255]
        data_min = data.min()
        data_max = data.max()
        if data_max > data_min:
            normalized = (data - data_min) / (data_max - data_min) * 255.0
        else:
            normalized = np.zeros_like(data)
    elif method == "zscore":
        # Z-score å½’ä¸€åŒ–ï¼Œç„¶åæ˜ å°„åˆ° [0, 255]
        mean = data.mean()
        std = data.std()
        if std > 0:
            normalized = (data - mean) / std
            # æ˜ å°„åˆ° [0, 255]ï¼Œå‡è®¾ Â±3Ïƒ èŒƒå›´
            normalized = np.clip((normalized + 3) / 6 * 255.0, 0, 255)
        else:
            normalized = np.zeros_like(data)
    else:
        raise ValueError(f"Unknown normalization method: {method}")

    return normalized.astype(np.uint8)


def _save_single_image_colormap(args_tuple):
    """
    ä¿å­˜å•å¼ å›¾ç‰‡çš„è¾…åŠ©å‡½æ•°ï¼ˆç”¨äºå¹¶å‘å¤„ç†ï¼‰

    Args:
        args_tuple: (img_data, output_path, cmap, idx)

    Returns:
        (idx, success)
    """
    img_data, output_path, cmap, idx = args_tuple

    try:
        # å•é€šé“ -> å½©è‰²
        if img_data.shape[0] == 1:
            # [0,255] â†’ [0,1]
            normalized = img_data[0] / 255.0
            colored = cmap(normalized)[:, :, :3]  # RGBA â†’ RGB
            colored = (colored * 255).astype(np.uint8)
            img_data = np.transpose(colored, (2, 0, 1))

        # è½¬æ¢ä¸º (H, W, C)
        img_data = np.transpose(img_data, (1, 2, 0))
        img = Image.fromarray(img_data, mode="RGB")
        img.save(output_path)
        return (idx, True)
    except Exception as e:
        return (idx, False, str(e))


def save_weather_images_colormap(
    data: np.ndarray,
    output_dir: Path,
    prefix: str = "sample",
    start_idx: int = 0,
    cmap_name: str = "coolwarm",
    n_workers: int = 4,
):
    """
    ä¿å­˜å¤©æ°”æ•°æ®ä¸ºå›¾ç‰‡ï¼ˆæ”¯æŒå½©è‰² colormapï¼Œå¹¶å‘ç‰ˆæœ¬ï¼‰

    Args:
        data: æ•°æ®ï¼Œshape (Time, C, H, W) æˆ– (C, H, W)
        output_dir: è¾“å‡ºç›®å½•
        prefix: æ–‡ä»¶åå‰ç¼€
        start_idx: èµ·å§‹ç´¢å¼•
        cmap_name: colormap åç§°
        n_workers: å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°
    """
    import matplotlib.cm as cm

    output_dir.mkdir(parents=True, exist_ok=True)

    # è·å– colormap
    cmap = cm.get_cmap(cmap_name)

    if data.ndim == 3:
        data = data[np.newaxis, ...]

    n_samples = len(data)

    # å‡†å¤‡ä»»åŠ¡åˆ—è¡¨
    tasks = []
    for i in range(n_samples):
        img_data = data[i]  # (C, H, W)
        output_path = output_dir / f"{prefix}_{start_idx + i:06d}.png"
        tasks.append((img_data, output_path, cmap, i))

    # å¹¶å‘ä¿å­˜
    if n_workers > 1:
        with ThreadPoolExecutor(max_workers=n_workers) as executor:
            futures = {
                executor.submit(_save_single_image_colormap, task): task[3]
                for task in tasks
            }

            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
            completed = 0
            failed = 0
            failed_indices = []
            start_time = time.time()

            with tqdm(
                total=n_samples,
                desc="ğŸ’¾ ä¿å­˜å›¾ç‰‡",
                unit="å¼ ",
                unit_scale=False,
                bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
            ) as pbar:
                for future in as_completed(futures):
                    result = future.result()
                    if len(result) == 2 and result[1]:
                        completed += 1
                    else:
                        failed += 1
                        failed_indices.append(result[0])
                        if len(result) > 2:
                            print(f"\nâš ï¸  ä¿å­˜å›¾ç‰‡ {result[0]} å¤±è´¥: {result[2]}")
                    pbar.update(1)

                    # æ›´æ–°åå¤„ç†ä¿¡æ¯
                    elapsed = time.time() - start_time
                    rate = completed / elapsed if elapsed > 0 else 0
                    pbar.set_postfix(
                        {"æˆåŠŸ": completed, "å¤±è´¥": failed, "é€Ÿåº¦": f"{rate:.1f} å¼ /ç§’"}
                    )

            if failed > 0:
                print(
                    f"\nâš ï¸  è­¦å‘Š: {failed} å¼ å›¾ç‰‡ä¿å­˜å¤±è´¥ (ç´¢å¼•: {failed_indices[:10]}{'...' if len(failed_indices) > 10 else ''})"
                )
    else:
        # å•çº¿ç¨‹æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        for i in tqdm(range(n_samples), desc="ä¿å­˜å›¾ç‰‡", unit="å¼ "):
            img_data = data[i]  # (C, H, W)

            # å•é€šé“ -> å½©è‰²
            if img_data.shape[0] == 1:
                # [0,255] â†’ [0,1]
                normalized = img_data[0] / 255.0
                colored = cmap(normalized)[:, :, :3]  # RGBA â†’ RGB
                colored = (colored * 255).astype(np.uint8)
                img_data = np.transpose(colored, (2, 0, 1))
            else:
                # å·²æ˜¯RGB
                pass

            # è½¬æ¢ä¸º (H, W, C)
            img_data = np.transpose(img_data, (1, 2, 0))
            img = Image.fromarray(img_data, mode="RGB")
            output_path = output_dir / f"{prefix}_{start_idx + i:06d}.png"
            img.save(output_path)


def save_weather_images(
    data: np.ndarray, output_dir: Path, prefix: str = "sample", start_idx: int = 0
):
    """
    ä¿å­˜å¤©æ°”æ•°æ®ä¸ºå›¾ç‰‡

    Args:
        data: æ•°æ®ï¼Œshape (Time, C, H, W) æˆ– (C, H, W)
        output_dir: è¾“å‡ºç›®å½•
        prefix: æ–‡ä»¶åå‰ç¼€
        start_idx: èµ·å§‹ç´¢å¼•
    """
    output_dir.mkdir(parents=True, exist_ok=True)

    # ç¡®ä¿æ˜¯4ç»´
    if data.ndim == 3:
        data = data[np.newaxis, ...]

    n_samples = len(data)

    for i in tqdm(range(n_samples), desc="ä¿å­˜å›¾ç‰‡"):
        img_data = data[i]  # (C, H, W)

        # å¦‚æœæ˜¯å•é€šé“ï¼Œè½¬æ¢ä¸º3é€šé“
        if img_data.shape[0] == 1:
            img_data = np.repeat(img_data, 3, axis=0)

        # è½¬æ¢ä¸º (H, W, C) æ ¼å¼
        img_data = np.transpose(img_data, (1, 2, 0))

        # ä¿å­˜ä¸ºPNG
        img = Image.fromarray(img_data, mode="RGB")
        output_path = output_dir / f"{prefix}_{start_idx + i:06d}.png"
        img.save(output_path)


def main():
    parser = argparse.ArgumentParser(description="å°† ERA5 å¤©æ°”æ•°æ®æ’å€¼å¹¶ä¿å­˜ä¸ºå›¾ç‰‡")

    # æ•°æ®å‚æ•°
    parser.add_argument(
        "--data-path",
        type=str,
        default="gs://weatherbench2/datasets/era5/1959-2022-6h-64x32_equiangular_conservative.zarr",
        help="ERA5 æ•°æ®è·¯å¾„ï¼ˆzarræ ¼å¼ï¼‰",
    )
    parser.add_argument(
        "--variable",
        type=str,
        default="2m_temperature",
        help="è¦æå–çš„å˜é‡åï¼ˆå¦‚ 2m_temperature, geopotential_500, total_precipitationï¼‰",
    )
    parser.add_argument(
        "--time-slice",
        type=str,
        default=None,
        help="æ—¶é—´åˆ‡ç‰‡ï¼Œæ ¼å¼: 2020-01-01:2020-12-31",
    )

    # å¤„ç†å‚æ•°
    parser.add_argument(
        "--target-size",
        type=int,
        nargs=2,
        default=[256, 256],
        help="ç›®æ ‡å›¾åƒå°ºå¯¸ [height width]ï¼Œé»˜è®¤ [256 256]",
    )
    parser.add_argument(
        "--n-channels",
        type=int,
        default=3,
        choices=[1, 3],
        help="è¾“å‡ºé€šé“æ•°ï¼ˆ1æˆ–3ï¼‰ï¼Œé»˜è®¤3",
    )
    parser.add_argument(
        "--normalization",
        type=str,
        default="minmax",
        choices=["minmax", "zscore"],
        help="å½’ä¸€åŒ–æ–¹æ³•ï¼Œé»˜è®¤ minmax",
    )
    parser.add_argument(
        "--norm-stats-path",
        type=str,
        default=None,
        help="å¯é€‰ï¼Œæä¾›å·²æœ‰çš„ normalization_stats.json æ–‡ä»¶ä»¥å¤ç”¨å½’ä¸€åŒ–å‚æ•°",
    )
    parser.add_argument(
        "--concurrent-stats",
        action="store_true",
        help="ä½¿ç”¨å¹¶å‘è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡ï¼ˆå¯¹äºå¤§æ•°æ®é›†å¯èƒ½æ›´å¿«ï¼‰",
    )

    # è¾“å‡ºå‚æ•°
    parser.add_argument(
        "--output-dir",
        type=str,
        default="weather_images",
        help="è¾“å‡ºç›®å½•",
    )
    parser.add_argument(
        "--n-samples",
        type=int,
        default=None,
        help="è¦å¤„ç†çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤å¤„ç†æ‰€æœ‰æ•°æ®ï¼‰",
    )
    parser.add_argument(
        "--prefix",
        type=str,
        default="sample",
        help="è¾“å‡ºæ–‡ä»¶åå‰ç¼€",
    )
    parser.add_argument(
        "--n-workers",
        type=int,
        default=4,
        help="å¹¶å‘å·¥ä½œçº¿ç¨‹/è¿›ç¨‹æ•°ï¼ˆç”¨äºå›¾ç‰‡ä¿å­˜å’Œæ•°æ®å¤„ç†ï¼‰ï¼Œé»˜è®¤4",
    )
    parser.add_argument(
        "--concurrent-interpolation",
        action="store_true",
        help="ä½¿ç”¨å¹¶å‘å¤„ç†æ•°æ®æ’å€¼ï¼ˆå¯¹äºå¤§æ•°æ®é›†å¯èƒ½æ›´å¿«ï¼‰",
    )

    args = parser.parse_args()

    # è®°å½•æ€»å¼€å§‹æ—¶é—´
    total_start_time = time.time()

    print("=" * 80)
    print("ERA5 å¤©æ°”æ•°æ® â†’ å›¾ç‰‡è½¬æ¢å·¥å…·")
    print("=" * 80)
    print(f"å¹¶å‘å·¥ä½œçº¿ç¨‹æ•°: {args.n_workers}")
    print(f"å¹¶å‘æ’å€¼: {'å¯ç”¨' if args.concurrent_interpolation else 'ç¦ç”¨'}")
    print(f"å¹¶å‘ç»Ÿè®¡é‡è®¡ç®—: {'å¯ç”¨' if args.concurrent_stats else 'ç¦ç”¨'}")
    print("=" * 80)

    # 1. åŠ è½½æ•°æ®
    load_start = time.time()
    print(f"\n[1/5] ğŸ“¥ åŠ è½½æ•°æ®: {args.data_path}")
    print(f"   å˜é‡: {args.variable}")

    try:
        with tqdm(desc="   æ‰“å¼€æ•°æ®æ–‡ä»¶", leave=False) as pbar:
            ds = xr.open_zarr(args.data_path)
            pbar.update(1)
    except Exception as e:
        print(f"   âŒ é”™è¯¯: æ— æ³•æ‰“å¼€æ•°æ®æ–‡ä»¶")
        print(f"   è¯·ç¡®ä¿å·²å®‰è£… gcsfs: pip install gcsfs")
        print(f"   é”™è¯¯è¯¦æƒ…: {e}")
        return

    # æ£€æŸ¥å˜é‡æ˜¯å¦å­˜åœ¨
    if args.variable not in ds.data_vars:
        print(f"   âŒ é”™è¯¯: å˜é‡ '{args.variable}' ä¸å­˜åœ¨")
        print(f"   å¯ç”¨å˜é‡: {list(ds.data_vars)[:10]}...")
        return

    # æ—¶é—´åˆ‡ç‰‡
    if args.time_slice:
        start, end = args.time_slice.split(":")
        with tqdm(desc="   åº”ç”¨æ—¶é—´åˆ‡ç‰‡", leave=False) as pbar:
            ds = ds.sel(time=slice(start, end))
            pbar.update(1)
        print(f"   æ—¶é—´èŒƒå›´: {start} è‡³ {end}")

    # è·å–å˜é‡æ•°æ®
    with tqdm(desc="   è¯»å–å˜é‡æ•°æ®", leave=False) as pbar:
        variable_data = ds[args.variable]
        data = variable_data.values  # (Time, H, W) æˆ– (Time, Lat, Lon)
        pbar.update(1)

    # ERA5 é»˜è®¤ç»´åº¦: (time, lat, lon)
    # æˆ‘ä»¬éœ€è¦è½¬æ¢ä¸º (time, height=lat, width=lon)
    if "latitude" in variable_data.dims and "longitude" in variable_data.dims:
        data = np.transpose(data, (0, 2, 1))  # äº¤æ¢ (lat, lon)
        print("   âš™ï¸ è‡ªåŠ¨è°ƒæ•´çº¬ç»åº¦ç»´åº¦é¡ºåº: (time, lon, lat)")

    # é™åˆ¶æ ·æœ¬æ•°é‡
    if args.n_samples is not None and args.n_samples < len(data):
        data = data[: args.n_samples]
        print(f"   âš™ï¸ é™åˆ¶æ ·æœ¬æ•°: {args.n_samples}")

    load_time = time.time() - load_start
    print(f"   åŸå§‹æ•°æ® shape: {data.shape}")
    print(f"   æ•°æ®èŒƒå›´: [{data.min():.2f}, {data.max():.2f}]")
    print(f"   æ•°æ®å•ä½: {variable_data.attrs.get('units', 'N/A')}")
    print(f"   âœ“ åŠ è½½å®Œæˆ (è€—æ—¶: {load_time:.2f}ç§’)")

    # 2. æ’å€¼åˆ°ç›®æ ‡å°ºå¯¸
    interp_start = time.time()
    print(f"\n[2/5] ğŸ”„ æ’å€¼åˆ°ç›®æ ‡å°ºå¯¸: {args.target_size}")
    if args.concurrent_interpolation:
        print(f"   ä½¿ç”¨å¹¶å‘å¤„ç†ï¼ˆå·¥ä½œçº¿ç¨‹æ•°: {args.n_workers}ï¼‰")
    data_processed = prepare_weather_data(
        data,
        n_channels=args.n_channels,
        target_size=tuple(args.target_size),
        n_workers=args.n_workers,
        use_concurrent=args.concurrent_interpolation,
    )
    interp_time = time.time() - interp_start
    print(f"   å¤„ç†å shape: {data_processed.shape}")
    print(f"   âœ“ æ’å€¼å®Œæˆ (è€—æ—¶: {interp_time:.2f}ç§’)")

    # 3. å½’ä¸€åŒ–
    existing_norm_stats = None
    norm_stats_path = None
    if args.norm_stats_path:
        norm_stats_path = Path(args.norm_stats_path)
        if not norm_stats_path.exists():
            print(f"   âŒ é”™è¯¯: æ‰¾ä¸åˆ°å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶ {norm_stats_path}")
            return
        try:
            with open(norm_stats_path, "r") as f:
                existing_norm_stats = json.load(f)
            print(f"   ä½¿ç”¨å¤–éƒ¨å½’ä¸€åŒ–å‚æ•°: {norm_stats_path}")
        except Exception as e:
            print(f"   âŒ é”™è¯¯: æ— æ³•è¯»å–å½’ä¸€åŒ–å‚æ•°æ–‡ä»¶: {e}")
            return

    normalization_method = (
        existing_norm_stats.get("method")
        if existing_norm_stats and existing_norm_stats.get("method")
        else args.normalization
    )

    if (
        existing_norm_stats
        and existing_norm_stats.get("method")
        and existing_norm_stats.get("method") != args.normalization
    ):
        print(
            f"   âš ï¸ æä¾›çš„å½’ä¸€åŒ–æ–¹æ³• {existing_norm_stats.get('method')} "
            f"ä¸å‘½ä»¤è¡Œå‚æ•° {args.normalization} ä¸ä¸€è‡´ï¼Œå°†ä½¿ç”¨å¤–éƒ¨å‚æ•°ä¸­çš„æ–¹æ³•ã€‚"
        )

    norm_start = time.time()
    print(f"\n[3/5] ğŸ“Š å…¨å±€å½’ä¸€åŒ–ï¼ˆæ–¹æ³•: {normalization_method}ï¼‰")

    if existing_norm_stats:
        print("   ä½¿ç”¨æä¾›çš„ç»Ÿè®¡é‡è¿›è¡Œå½’ä¸€åŒ–")
    else:
        if args.concurrent_stats and args.n_workers > 1:
            print(f"   ä½¿ç”¨å¹¶å‘è®¡ç®—ç»Ÿè®¡é‡ï¼ˆå·¥ä½œçº¿ç¨‹æ•°: {args.n_workers}ï¼‰")
            stats = compute_normalization_stats_concurrent(
                data_processed,
                method=normalization_method,
                n_workers=args.n_workers,
            )
        else:
            print("   è®¡ç®—ç»Ÿè®¡é‡...", end="", flush=True)
            if normalization_method == "minmax":
                stats = {"min": data_processed.min(), "max": data_processed.max()}
            elif normalization_method == "zscore":
                stats = {
                    "mean": data_processed.mean(),
                    "std": data_processed.std(),
                }
            print(" âœ“")

    if normalization_method == "minmax":
        if existing_norm_stats:
            global_min = existing_norm_stats.get("original_min")
            global_max = existing_norm_stats.get("original_max")
            if global_min is None or global_max is None:
                print("   âŒ é”™è¯¯: æä¾›çš„å½’ä¸€åŒ–å‚æ•°ç¼ºå°‘ original_min æˆ– original_max")
                return
            print(
                f"   å¤–éƒ¨å…¨å±€èŒƒå›´: [{float(global_min):.4f}, {float(global_max):.4f}]"
            )
        else:
            global_min = stats["min"]
            global_max = stats["max"]
            print(f"   å…¨å±€èŒƒå›´: [{global_min:.4f}, {global_max:.4f}]")

        # å½’ä¸€åŒ–åˆ° [0, 255]
        print("   æ‰§è¡Œå½’ä¸€åŒ–...", end="", flush=True)
        denom = (global_max - global_min) + 1e-8
        data_normalized = (data_processed - global_min) / denom
        data_normalized = np.clip(data_normalized * 255.0, 0, 255).astype(np.uint8)
        print(" âœ“")

        # ä¿å­˜å…¨å±€ç»Ÿè®¡é‡ç”¨äºåç»­åå½’ä¸€åŒ–
        global_mean = None
        global_std = None

    elif normalization_method == "zscore":
        if existing_norm_stats:
            global_mean = existing_norm_stats.get("original_mean")
            global_std = existing_norm_stats.get("original_std")
            if global_mean is None or global_std is None:
                print("   âŒ é”™è¯¯: æä¾›çš„å½’ä¸€åŒ–å‚æ•°ç¼ºå°‘ original_mean æˆ– original_std")
                return
            print(
                f"   å¤–éƒ¨å…¨å±€å‡å€¼: {float(global_mean):.4f}, æ ‡å‡†å·®: {float(global_std):.4f}"
            )
        else:
            global_mean = stats["mean"]
            global_std = stats["std"]
            print(f"   å…¨å±€å‡å€¼: {global_mean:.4f}, æ ‡å‡†å·®: {global_std:.4f}")

        # Z-score å½’ä¸€åŒ–å†æ˜ å°„åˆ° [0, 255]
        print("   æ‰§è¡Œå½’ä¸€åŒ–...", end="", flush=True)
        data_normalized = (data_processed - global_mean) / (global_std + 1e-8)
        data_normalized = np.clip((data_normalized + 3) / 6 * 255.0, 0, 255).astype(
            np.uint8
        )
        print(" âœ“")

        # ä¿å­˜å…¨å±€ç»Ÿè®¡é‡ç”¨äºåç»­åå½’ä¸€åŒ–
        global_min = None
        global_max = None

    else:
        raise ValueError(f"Unknown normalization method: {normalization_method}")

    norm_time = time.time() - norm_start
    print(f"   å½’ä¸€åŒ–åèŒƒå›´: [{data_normalized.min()}, {data_normalized.max()}]")
    print(f"   âœ“ å½’ä¸€åŒ–å®Œæˆ (è€—æ—¶: {norm_time:.2f}ç§’)")

    # 4. ä¿å­˜å½’ä¸€åŒ–å‚æ•°
    save_stats_start = time.time()
    print(f"\n[4/5] ğŸ’¾ ä¿å­˜å½’ä¸€åŒ–å‚æ•°")
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯
    norm_stats = {
        "method": normalization_method,
        "variable": args.variable,
        "original_min": float(global_min) if normalization_method == "minmax" else None,
        "original_max": float(global_max) if normalization_method == "minmax" else None,
        "original_mean": (
            float(global_mean) if normalization_method == "zscore" else None
        ),
        "original_std": float(global_std) if normalization_method == "zscore" else None,
    }

    with open(output_dir / "normalization_stats.json", "w") as f:
        json.dump(norm_stats, f, indent=2)

    save_stats_time = time.time() - save_stats_start
    print(f"   âœ“ å½’ä¸€åŒ–å‚æ•°å·²ä¿å­˜: {output_dir / 'normalization_stats.json'}")
    print(f"   âœ“ ä¿å­˜å®Œæˆ (è€—æ—¶: {save_stats_time:.2f}ç§’)")

    # 5. ä¿å­˜å›¾ç‰‡
    save_img_start = time.time()
    print(f"\n[5/5] ğŸ’¾ ä¿å­˜å›¾ç‰‡åˆ°: {args.output_dir}")
    print(f"   ä½¿ç”¨å¹¶å‘ä¿å­˜ï¼ˆå·¥ä½œçº¿ç¨‹æ•°: {args.n_workers}ï¼‰")
    save_weather_images_colormap(
        data_normalized,
        output_dir,
        prefix=args.prefix,
        start_idx=0,
        cmap_name="turbo",
        n_workers=args.n_workers,
    )
    save_img_time = time.time() - save_img_start

    # æ€»è€—æ—¶ç»Ÿè®¡
    total_time = time.time() - total_start_time

    print(f"\n{'='*80}")
    print("âœ… å¤„ç†å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡:")
    print(f"   å…±ä¿å­˜ {len(data_normalized)} å¼ å›¾ç‰‡")
    print(f"   è¾“å‡ºç›®å½•: {output_dir.absolute()}")
    print(f"   å›¾ç‰‡å°ºå¯¸: {args.target_size}")
    print(f"   é€šé“æ•°: {args.n_channels}")
    print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡:")
    print(f"   æ•°æ®åŠ è½½: {load_time:.2f}ç§’")
    print(f"   æ•°æ®æ’å€¼: {interp_time:.2f}ç§’")
    print(f"   æ•°æ®å½’ä¸€åŒ–: {norm_time:.2f}ç§’")
    print(f"   ä¿å­˜å‚æ•°: {save_stats_time:.2f}ç§’")
    print(f"   ä¿å­˜å›¾ç‰‡: {save_img_time:.2f}ç§’")
    print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.2f}åˆ†é’Ÿ)")
    print(f"   å¹³å‡é€Ÿåº¦: {len(data_normalized)/total_time:.2f} å¼ /ç§’")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()
